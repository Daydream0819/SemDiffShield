import torch
import numpy as np
from PIL import Image
from diffusers import StableDiffusionPipeline, DDIMScheduler
import argparse
import os
from tqdm import tqdm
import torch
import torch.nn as nn
import torch
import numpy as np
from PIL import Image
from diffusers import StableDiffusionPipeline, DDIMScheduler
import argparse
import os
import csv



# ================= é…ç½® =================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
DTYPE = torch.float32
MODEL_ID = "runwayml/stable-diffusion-v1-5"

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# ================= 1. æ ¸å¿ƒæ•°å­¦å·¥å…· =================

def get_keys_from_seed(key, length, device):
    """ ç”Ÿæˆç©ºé—´ç½®ä¹±çš„ç´¢å¼• """
    g = torch.Generator(device="cpu").manual_seed(key)
    perm_idx = torch.randperm(length, generator=g).to(device)
    inv_perm_idx = torch.argsort(perm_idx)
    return perm_idx, inv_perm_idx


def spatial_permute(latents, key, inverse=False):
    """ ç©ºé—´ç½®ä¹±: æ‰“ç¢ç»“æ„ """
    b, c, h, w = latents.shape
    latents = latents.reshape(b, c, -1)
    perm_idx, inv_perm_idx = get_keys_from_seed(key, h * w, latents.device)

    if not inverse:
        latents = latents[:, :, perm_idx]
    else:
        latents = latents[:, :, inv_perm_idx]

    return latents.reshape(b, c, h, w)


def get_orthogonal_matrix(key, dim, device):
    gen = torch.Generator(device="cpu").manual_seed(key)
    H = torch.randn(dim, dim, generator=gen)
    Q, R = torch.linalg.qr(H)
    return Q.to(device).to(DTYPE)


def orthogonal_transform(latents, key, inverse=False):
    """ é€šé“æ—‹è½¬ """
    b, c, h, w = latents.shape
    latents_perm = latents.permute(0, 2, 3, 1)
    Q = get_orthogonal_matrix(key, c, latents.device)
    if not inverse:
        res = torch.matmul(latents_perm, Q.t())
    else:
        res = torch.matmul(latents_perm, Q)
    return res.permute(0, 3, 1, 2)


def get_secret_trajectory(key, num_steps):
    """ ç”Ÿæˆç§˜å¯†æ—¶é—´æ­¥ """
    np.random.seed(key)
    full_range = np.arange(0, 1000)
    chosen = np.random.choice(full_range[1:], size=num_steps - 1, replace=False)
    chosen = np.sort(chosen)[::-1]
    chosen = np.append(chosen, 0)
    return torch.from_numpy(chosen.copy()).long().to(DEVICE)


def get_step_bound_embeds(base_embeds, t_val, secret_key):
    """ SL-DSM æ‰°åŠ¨ """
    step_seed = int(secret_key) + int(t_val.item()) * 777
    gen = torch.Generator(device="cpu").manual_seed(step_seed)
    noise = torch.randn(base_embeds.shape, generator=gen).to(DEVICE).to(DTYPE)
    scale = 8.0
    return base_embeds + scale * noise


def statistical_correction(latents, target_mean=0.0, target_std=1.0):
    curr_mean = latents.mean()
    curr_std = latents.std()
    latents = (latents - curr_mean) / curr_std
    latents = latents * target_std + target_mean
    return latents, curr_mean, curr_std

def pil_to_tensor_01(img_pil: Image.Image) -> torch.Tensor:
    """PIL -> torch tensor in [0,1], shape [1,3,H,W], float32, on CPU"""
    img = img_pil.convert("RGB")
    arr = np.array(img).astype(np.float32) / 255.0
    ten = torch.from_numpy(arr).permute(2, 0, 1).unsqueeze(0)  # 1,3,H,W
    return ten

@torch.no_grad()
def compute_mse_psnr(x: torch.Tensor, y: torch.Tensor):
    """x,y: [1,3,H,W] in [0,1]"""
    mse = torch.mean((x - y) ** 2).item()
    psnr = float("inf") if mse == 0 else 10.0 * np.log10(1.0 / mse)
    return mse, psnr

def try_init_ssim():
    """Return a callable ssim_fn(x,y)->float or None."""
    try:
        from pytorch_msssim import ssim as ssim_fn  # expects [0,1]
        def _ssim(x, y):
            return float(ssim_fn(x, y, data_range=1.0, size_average=True).item())
        return _ssim
    except Exception:
        return None

def try_init_lpips(device):
    """Return lpips_model(x,y)->float or None. Expects [-1,1]."""
    try:
        import lpips
        model = lpips.LPIPS(net="alex").to(device)
        model.eval()
        @torch.no_grad()
        def _lpips(x01, y01):
            # [0,1] -> [-1,1]
            x = x01.to(device) * 2 - 1
            y = y01.to(device) * 2 - 1
            v = model(x, y)
            return float(v.mean().item())
        return _lpips
    except Exception:
        return None

def try_init_clip(device):
    """
    Return clip_sim(x01,y01)->float or None.
    Uses transformers CLIPModel/CLIPProcessor.
    """
    try:
        from transformers import CLIPProcessor, CLIPModel
        model_id = "openai/clip-vit-base-patch32"
        model = CLIPModel.from_pretrained(model_id).to(device)
        proc = CLIPProcessor.from_pretrained(model_id)

        model.eval()

        @torch.no_grad()
        def _clip_sim(x_pil: Image.Image, y_pil: Image.Image):
            inputs = proc(images=[x_pil.convert("RGB"), y_pil.convert("RGB")],
                          return_tensors="pt")
            inputs = {k: v.to(device) for k, v in inputs.items()}
            feats = model.get_image_features(**inputs)  # [2, D]
            feats = feats / feats.norm(dim=-1, keepdim=True)
            sim = torch.sum(feats[0] * feats[1]).item()  # cosine
            return float(sim)

        return _clip_sim
    except Exception:
        return None
# ================= 2. é«˜ç²¾åº¦ Pipeline =================

def plot_curve(xs, ys, title, xlabel, ylabel, save_path):
    import matplotlib.pyplot as plt
    plt.figure()
    plt.plot(xs, ys, marker="o")
    plt.title(title)
    plt.xlabel(xlabel)
    plt.ylabel(ylabel)
    plt.grid(True, linestyle="--", linewidth=0.5)
    plt.tight_layout()
    plt.savefig(save_path, dpi=200)
    plt.close()

def parse_snr_list(s: str):
    # e.g. "3,5,7,9,11"
    parts = [p.strip() for p in s.split(",") if p.strip() != ""]
    return [float(p) for p in parts]
    
class CryptoPipeline:
    def __init__(self, num_steps=150):
        self.scheduler = DDIMScheduler.from_pretrained(MODEL_ID, subfolder="scheduler")
        self.pipe = StableDiffusionPipeline.from_pretrained(
            MODEL_ID, scheduler=self.scheduler, safety_checker=None, torch_dtype=DTYPE
        ).to(DEVICE)
        self.pipe.unet.to(DTYPE)

        self.scheduler.set_timesteps(num_steps)
        self.num_steps = num_steps
        self.alphas_cumprod = self.scheduler.alphas_cumprod.to(DEVICE).to(DTYPE)
        self.null_embeds = self.encode_text("")

    def encode_text(self, prompt):
        text_input = self.pipe.tokenizer(
            [prompt], padding="max_length", max_length=self.pipe.tokenizer.model_max_length,
            truncation=True, return_tensors="pt"
        )
        with torch.no_grad():
            return self.pipe.text_encoder(text_input.input_ids.to(DEVICE))[0].to(DTYPE)

    @torch.no_grad()
    def run_ddim_loop(self, latents, trajectory, prompt_embeds, key=None, is_inversion=False, use_sl_dsm=False,
                      guidance_scale=1.0, desc=""):
        if is_inversion:
            timesteps = torch.sort(trajectory)[0]
        else:
            timesteps = torch.sort(trajectory, descending=True)[0]

        curr_latents = latents.clone()

        # print(f"ğŸ”„ {desc} | SL-DSM={use_sl_dsm}")
        if guidance_scale > 1.0:
            uncond_embeds = self.null_embeds.repeat(latents.shape[0], 1, 1)
            text_embeds = prompt_embeds.repeat(latents.shape[0], 1, 1)

        for i in tqdm(range(len(timesteps) - 1), desc=f"{desc} (CFG={guidance_scale})", leave=False):
            t_curr = timesteps[i]
            t_next = timesteps[i + 1]

            # 1. Condition å¤„ç†
            current_cond = prompt_embeds
            if use_sl_dsm and key is not None:
                current_cond = get_step_bound_embeds(prompt_embeds, t_curr, key)
                if guidance_scale > 1.0: text_embeds = current_cond
            # 2. é¢„æµ‹å™ªå£°
            if guidance_scale > 1.0:
                latent_input = torch.cat([curr_latents] * 2)
                # è¿™é‡Œçš„ text_embeds å¯èƒ½æ˜¯çº¯å‡€çš„(ä¼ªè£…å±‚)ä¹Ÿå¯èƒ½æ˜¯æ‰°åŠ¨çš„(å¦‚æœç”¨äº†SLDSM)
                combined_embeds = torch.cat([uncond_embeds, text_embeds])

                noise_pred_combined = self.pipe.unet(latent_input, t_curr, encoder_hidden_states=combined_embeds).sample
                noise_pred_uncond, noise_pred_text = noise_pred_combined.chunk(2)

                noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)
            else:
                noise_pred = self.pipe.unet(curr_latents, t_curr, encoder_hidden_states=current_cond).sample
            # 3. å¯¹ç§° DDIM å…¬å¼
            alpha_curr = self.alphas_cumprod[t_curr]
            alpha_next = self.alphas_cumprod[t_next]
            beta_curr = 1 - alpha_curr
            beta_next = 1 - alpha_next

            pred_x0 = (curr_latents - beta_curr ** 0.5 * noise_pred) / alpha_curr ** 0.5

            if is_inversion:
                curr_latents = alpha_next ** 0.5 * pred_x0 + beta_next ** 0.5 * noise_pred
            else:
                curr_latents = alpha_next ** 0.5 * pred_x0 + beta_next ** 0.5 * noise_pred

        return curr_latents

    def image2latent(self, img_path):
        """ è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º Latent (Sender/Receiver é€šç”¨) """
        if not os.path.exists(img_path):
            print(f"âš ï¸ å›¾ç‰‡ä¸å­˜åœ¨ï¼Œç”Ÿæˆç°å›¾: {img_path}")
            Image.new('RGB', (512, 512), (128, 128, 128)).save(img_path)

        img = Image.open(img_path).convert("RGB").resize((512, 512))
        img = np.array(img).astype(np.float32) / 127.5 - 1.0
        img = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(DEVICE).to(DTYPE)

        # ä½¿ç”¨ latent_dist.mean è·å–ç¡®å®šæ€§ç‰¹å¾
        with torch.no_grad():
            latents = self.pipe.vae.encode(img).latent_dist.mean * self.pipe.vae.config.scaling_factor
        return latents

    def latent2image(self, latents, save_path):
        """ Latent è½¬å›¾ç‰‡å¹¶ä¿å­˜ """
        with torch.no_grad():
            latents = latents / self.pipe.vae.config.scaling_factor
            image = self.pipe.vae.decode(latents).sample
            image = (image / 2 + 0.5).clamp(0, 1).cpu().permute(0, 2, 3, 1).detach().numpy()[0]
            Image.fromarray((image * 255).astype(np.uint8)).save(save_path)


    def tensor2latent(pipeline, img_tensor):
        """
        ä¸“é—¨ç”¨äºå¤„ç† DeepJSCC è¾“å‡ºçš„ Tensor ç›´æ¥è½¬ Latent
        img_tensor: [1, 3, H, W], èŒƒå›´ [0, 1]
        """
        # 1. æ ¼å¼å¯¹é½
        # DeepJSCC è¾“å‡ºé€šå¸¸æ˜¯ [0, 1]ï¼Œä½† VAE éœ€è¦ [-1, 1]
        # å…¬å¼: x * 2 - 1
        input_tensor = img_tensor * 2.0 - 1.0

        # 2. ç¡®ä¿è®¾å¤‡ä¸€è‡´
        input_tensor = input_tensor.to(pipeline.pipe.device, dtype=pipeline.pipe.unet.dtype)

        # 3. VAE ç¼–ç 
        with torch.no_grad():
            # ç¼–ç å¾—åˆ°åˆ†å¸ƒ
            dist = pipeline.pipe.vae.encode(input_tensor).latent_dist
            # å–å‡å€¼å¹¶ç¼©æ”¾ (è¿™æ˜¯ Stable Diffusion çš„æ ‡å‡†åšæ³•)
            latents = dist.mean * pipeline.pipe.vae.config.scaling_factor

        return latents


# ================= 3. ä¸»ç¨‹åº =================

def main():
    # 1. å®šä¹‰å‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="è¿è¡Œéšå†™åŠ å¯†/è§£å¯†æµç¨‹")
    
    # å®šä¹‰å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument("--steps", type=int, default=150, help="é‡‡æ ·æ­¥æ•°")
    parser.add_argument("--key", type=int, default=2024, help="åŠ å¯†å¯†é’¥")
    parser.add_argument("--stego_cfg", type=float, default=3, help="ä¼ªè£…ç”Ÿæˆçš„å¼•å¯¼ç³»æ•°")
    parser.add_argument("--jscc_snr", type=float, default=15.0)
    parser.add_argument("--jscc_ckpt", type=str, default="./ckpt_latent/jscc_latent_best15.pkl") 
    parser.add_argument("--jscc_c", type=int, default=32)
    # æ ¸å¿ƒä¿®æ”¹ç‚¹ï¼šè¿™é‡Œå®šä¹‰äº†å‚æ•°ï¼Œå‘½ä»¤è¡Œè¾“å…¥æ—¶å°±ä¼šè¦†ç›– default å€¼
    parser.add_argument("--image_path", type=str, default="example/data1/flickr_dog_000054.jpg", help="åŸå§‹å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--image", type=str, default="11068", help="åŸå§‹å›¾ç‰‡è·¯å¾„")
    parser.add_argument("--public_emb", type=str, default="a black dog", help="å…¬å¼€çš„ä¼ªè£…æç¤ºè¯")
    parser.add_argument("--snr_list", type=str, default="15")
    parser.add_argument("--sweep",default= True, action="store_true", help="å¯ç”¨ SNR sweep è¯„æµ‹å¹¶ç”»æ›²çº¿")
    parser.add_argument("--out_dir", type=str, default="./snr_eval", help="è¾“å‡ºç›®å½•")
    
    # è§£æå‚æ•°
    args = parser.parse_args()
    os.makedirs(args.out_dir+args.image, exist_ok=True)
    
    # 2. è®¾å¤‡è®¾ç½® (ç§»åˆ°æœ€å‰é¢)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # 3. æ£€æŸ¥åŸå›¾æ˜¯å¦å­˜åœ¨ (é˜²æ­¢è·¯å¾„è¾“é”™æŠ¥é”™)
    if not os.path.exists(args.image_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°å›¾ç‰‡æ–‡ä»¶ -> {args.image_path}")
        sys.exit(1)

    # 4. åˆå§‹åŒ– Pipeline
    print(">>> åˆå§‹åŒ– Pipeline...")
    pipeline = CryptoPipeline(num_steps=args.steps)


    # ã€é‡è¦ã€‘ä¸€å®šè¦æŠŠæ¨¡å‹ç§»åŠ¨åˆ° GPUï¼Œå¦åˆ™ä¼šåœ¨ CPU è·‘ï¼Œææ…¢ï¼
    if hasattr(pipeline, 'to'):
        pipeline.to(device)

    # 5. Prompt è®¾ç½® (ä½¿ç”¨ args ä¸­çš„å‚æ•°)
    print(f">>> æ­£åœ¨ç¼–ç  Prompt: '{args.public_emb}'")
    secret_embeds = pipeline.encode_text("") 
    public_embeds = pipeline.encode_text(args.public_emb) # è¿™é‡Œä¼šä½¿ç”¨ä½ å‘½ä»¤è¡Œè¾“å…¥çš„ prompt
    public_embeds_wrong = pipeline.encode_text("a black bear")

    # 6. è¯»å–å›¾åƒ (ä½¿ç”¨ args ä¸­çš„è·¯å¾„)
    z0 = pipeline.image2latent(args.image_path)


    # ========================================================
    # [Sender] åŠ å¯†æµç¨‹
    # ========================================================
    print("\n>>> [Sender] åŠ å¯†æµç¨‹...")

    # 1. åŠ å¯†åæ¼”
    standard_traj = pipeline.scheduler.timesteps
    secret_traj = get_secret_trajectory(args.key, args.steps)

    zT_encrypted = pipeline.run_ddim_loop(
        z0,
        secret_traj,
        secret_embeds,
        key=args.key,
        is_inversion=True,
        use_sl_dsm=True,
        guidance_scale=1.0,
        desc="1.åŠ å¯†åæ¼”"
    )

    # 2. ç©ºé—´ç½®ä¹± + æ—‹è½¬
    z_perm = spatial_permute(zT_encrypted, args.key, inverse=False)
    z_enc = orthogonal_transform(z_perm, args.key, inverse=False)

    # 3. ç»Ÿè®¡çŸ«æ­£ (è®°å½• old_mean/std)
    z_enc_norm, old_mean, old_std = statistical_correction(z_enc)

    # 4. ä¼ªè£…ç”Ÿæˆ
    z_stego = pipeline.run_ddim_loop(
        z_enc_norm,
        standard_traj,
        public_embeds,
        is_inversion=False,
        use_sl_dsm=False,
        guidance_scale=args.stego_cfg,
        desc="3.ä¼ªè£…ç”Ÿæˆ"
    )
    stego_path = os.path.join(args.out_dir+args.image, "stego_final.png")
    pipeline.latent2image(z_stego, stego_path)
    print(f"âœ… [Sender] ä¼ªè£…å›¾å·²ä¿å­˜: {stego_path}")
    # ---------------- Receiverï¼ˆæ­£ç¡® keyï¼‰----------------
    hat_z_enc_norm = pipeline.run_ddim_loop(
        z_stego, standard_traj, public_embeds_wrong,
        is_inversion=True, use_sl_dsm=False,
        guidance_scale=args.stego_cfg, desc="1.æ ‡å‡†åæ¼”"
    )

    hat_z_enc = hat_z_enc_norm * old_std + old_mean
    hat_z_perm = orthogonal_transform(hat_z_enc, args.key, inverse=True)
    hat_zT = spatial_permute(hat_z_perm, args.key, inverse=True)

    secret_traj_rec = get_secret_trajectory(args.key, args.steps)
    hat_z0 = pipeline.run_ddim_loop(
        hat_zT, secret_traj_rec, secret_embeds,
        key=args.key, is_inversion=False, use_sl_dsm=True,
        guidance_scale=1.0, desc="4.åŠ å¯†æ¢å¤"
    )

    # ä¿å­˜æ¢å¤å›¾
    rec_path = os.path.join(args.out_dir+args.image, f"rec_final_snr{19:02d}.png")
    pipeline.latent2image(hat_z0, rec_path)
    print(f"âœ… è¾“å‡ºå·²ä¿å­˜: {rec_path}")
    
    return
   

if __name__ == "__main__":
    main()